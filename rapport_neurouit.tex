\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Configuration de la page
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

% Configuration des couleurs pour le code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=C++
}

\lstset{style=mystyle}

% Configuration des titres
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}
{\chaptertitlename\ \thechapter}{20pt}{\Huge}

\titlespacing*{\chapter}{0pt}{50pt}{40pt}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={NeuroUIT - Rapport de Projet},
    pdfauthor={Équipe NeuroUIT},
    pdfsubject={Simulation des Réseaux de Neurones Artificiels}
}

% Informations du document
\title{Simulation des Réseaux de Neurones Artificiels\\NeuroUIT}
\author{Équipe NeuroUIT\\Ait Aissa Rachid, Benfaress Ziad, Saliani Bouchaib,\\Flahi Sara, Bellihy Ibtissam}
\date{Année Universitaire 2025/2026}

\begin{document}

% Page de garde
\maketitle
\thispagestyle{empty}

\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
\textbf{\Large DÉDICACE}
\end{center}
\vspace{2cm}

Nous dédions ce travail à tous ceux qui croient en l'éducation et en la transmission du savoir. Ce projet représente notre engagement envers l'apprentissage de l'intelligence artificielle et notre volonté de contribuer à la compréhension des réseaux de neurones artificiels.

À nos familles qui nous ont soutenus tout au long de ce parcours académique.

À notre professeur, Pr. Mohamed Daoudi, pour son encadrement précieux et ses conseils éclairés.

\vspace*{\fill}

\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
\textbf{\Large REMERCIEMENTS}
\end{center}
\vspace{2cm}

Nous tenons à exprimer notre profonde gratitude à toutes les personnes qui ont contribué à la réalisation de ce projet.

En premier lieu, nous remercions sincèrement \textbf{Pr. Mohamed Daoudi}, professeur à la Faculté des Sciences de Kénitra (FSK), pour son encadrement exceptionnel, sa disponibilité et ses précieux conseils tout au long de ce projet. Son expertise dans le domaine de l'intelligence artificielle a été d'une aide inestimable.

Nous remercions également l'Université Ibn Tofail de Kénitra et la Faculté des Sciences de Kénitra pour nous avoir offert l'opportunité de réaliser ce projet dans le cadre du Master Intelligence Artificielle et Objets Connectés.

Notre gratitude va également à tous les membres de l'équipe NeuroUIT :
\begin{itemize}
    \item Ait Aissa Rachid
    \item Benfaress Ziad
    \item Saliani Bouchaib
    \item Flahi Sara
    \item Bellihy Ibtissam
\end{itemize}

pour leur collaboration, leur dévouement et leur esprit d'équipe qui ont rendu possible la réalisation de ce projet.

Enfin, nous remercions tous ceux qui, de près ou de loin, ont contribué à l'aboutissement de ce travail.

\vspace*{\fill}

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables

\newpage
\chapter*{Introduction Générale}
\addcontentsline{toc}{chapter}{Introduction Générale}

L'intelligence artificielle (IA) constitue aujourd'hui l'un des domaines les plus dynamiques et prometteurs de l'informatique moderne. Parmi les techniques les plus puissantes de l'IA, les réseaux de neurones artificiels occupent une place centrale, inspirés du fonctionnement du cerveau biologique pour résoudre des problèmes complexes de classification, de régression et de reconnaissance de patterns.

Le projet \textbf{NeuroUIT} (Neuro + UI + Tool) s'inscrit dans cette perspective en proposant un simulateur complet et pédagogique de réseaux de neurones artificiels. Développé en C++ avec le framework Qt, ce projet vise à offrir une plateforme interactive permettant de comprendre, visualiser et expérimenter avec les réseaux de neurones multicouches.

Ce rapport présente de manière détaillée la conception, l'implémentation et les résultats de ce projet académique réalisé dans le cadre du Master Intelligence Artificielle et Objets Connectés à l'Université Ibn Tofail de Kénitra, sous la supervision du Pr. Mohamed Daoudi.

Le document est structuré en cinq chapitres principaux :

\begin{itemize}
    \item \textbf{Chapitre I} : Contexte général et concepts fondamentaux des réseaux de neurones artificiels
    \item \textbf{Chapitre II} : Analyse et spécification du projet, incluant le cahier des charges et les technologies utilisées
    \item \textbf{Chapitre III} : Modélisation et conception du système avec les diagrammes UML et l'architecture logicielle
    \item \textbf{Chapitre IV} : Implémentation et réalisation technique du système
    \item \textbf{Chapitre V} : Simulation, expérimentation et résultats avec captures d'écran détaillées
\end{itemize}

Ce travail représente une contribution pédagogique significative, permettant aux étudiants et aux chercheurs de mieux comprendre le fonctionnement interne des réseaux de neurones à travers une interface graphique intuitive et des visualisations interactives.

\newpage
\chapter{Contexte Général et Concepts Fondamentaux}

\section{Contexte du Projet}

Le projet NeuroUIT s'inscrit dans un contexte académique et pédagogique visant à approfondir la compréhension des réseaux de neurones artificiels. Dans un monde où l'intelligence artificielle transforme de nombreux secteurs, il devient essentiel de disposer d'outils pédagogiques permettant de visualiser et de comprendre le fonctionnement interne de ces systèmes.

Le développement de ce simulateur répond à plusieurs besoins :
\begin{itemize}
    \item \textbf{Besoin pédagogique} : Offrir un outil d'apprentissage interactif pour les étudiants en intelligence artificielle
    \item \textbf{Besoin de compréhension} : Visualiser le fonctionnement des réseaux de neurones de manière intuitive
    \item \textbf{Besoin expérimental} : Permettre l'expérimentation avec différents types de réseaux et de datasets
    \item \textbf{Besoin de recherche} : Fournir une base pour des travaux futurs en apprentissage automatique
\end{itemize}

\section{Intelligence Artificielle et Réseaux de Neurones Artificiels}

\subsection{Historique et Évolution}

L'intelligence artificielle moderne trouve ses racines dans les travaux de pionniers comme Alan Turing, qui a posé les fondements théoriques de la computation. Les réseaux de neurones artificiels, quant à eux, ont été inspirés par les travaux de Warren McCulloch et Walter Pitts en 1943, qui ont proposé le premier modèle mathématique d'un neurone artificiel.

Le perceptron, développé par Frank Rosenblatt en 1957, a marqué une étape importante. Cependant, c'est avec l'algorithme de rétropropagation (backpropagation) développé dans les années 1980 que les réseaux multicouches ont véritablement pris leur essor.

\subsection{Concepts Fondamentaux}

Un réseau de neurones artificiel est un modèle computationnel inspiré du système nerveux biologique. Il est composé de :

\begin{enumerate}
    \item \textbf{Neurones artificiels} : Unités de calcul qui reçoivent des entrées, les traitent et produisent une sortie
    \item \textbf{Connexions} : Liens entre neurones avec des poids associés
    \item \textbf{Couches} : Groupes de neurones organisés hiérarchiquement
    \item \textbf{Fonctions d'activation} : Transformations non-linéaires appliquées aux sorties des neurones
\end{enumerate}

\subsection{Principe de Fonctionnement}

Le fonctionnement d'un réseau de neurones se base sur deux phases principales :

\subsubsection{Propagation Avant (Forward Propagation)}

La propagation avant consiste à faire passer les données d'entrée à travers le réseau :
\begin{enumerate}
    \item Les données sont présentées à la couche d'entrée
    \item Chaque neurone calcule une somme pondérée de ses entrées : $z = \sum_{i} w_i \cdot x_i + b$
    \item Une fonction d'activation est appliquée : $a = f(z)$
    \item Le résultat est transmis à la couche suivante
    \item Le processus se répète jusqu'à la couche de sortie
\end{enumerate}

\subsubsection{Rétropropagation (Backpropagation)}

La rétropropagation est l'algorithme d'apprentissage qui permet d'ajuster les poids :
\begin{enumerate}
    \item L'erreur est calculée entre la sortie prédite et la sortie attendue
    \item L'erreur est propagée en arrière à travers le réseau
    \item Les gradients sont calculés pour chaque poids
    \item Les poids sont mis à jour selon la règle : $w_{new} = w_{old} - \eta \cdot \frac{\partial E}{\partial w}$
\end{enumerate}

où $\eta$ est le taux d'apprentissage et $E$ est la fonction d'erreur.

\section{Types de Réseaux de Neurones}

\subsection{Réseaux Multicouches (MLP - Multi-Layer Perceptron)}

Les réseaux multicouches, également appelés perceptrons multicouches, sont composés de :
\begin{itemize}
    \item Une \textbf{couche d'entrée} : Reçoit les données brutes
    \item Une ou plusieurs \textbf{couches cachées} : Effectuent les transformations complexes
    \item Une \textbf{couche de sortie} : Produit le résultat final
\end{itemize}

C'est le type de réseau implémenté dans NeuroUIT.

\subsection{Fonctions d'Activation}

Les fonctions d'activation introduisent la non-linéarité dans le réseau. NeuroUIT implémente quatre types :

\subsubsection{Sigmoïde}
$$f(x) = \frac{1}{1 + e^{-x}}$$
Plage : $[0, 1]$. Utilisée pour la classification binaire.

\subsubsection{Tanh (Tangente Hyperbolique)}
$$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
Plage : $[-1, 1]$. Permet une normalisation centrée.

\subsubsection{ReLU (Rectified Linear Unit)}
$$f(x) = \max(0, x)$$
Plage : $[0, +\infty[$. Très utilisée dans les réseaux profonds.

\subsubsection{Linéaire}
$$f(x) = x$$
Pas de transformation. Utilisée pour la régression.

\section{Domaines d'Application des Réseaux de Neurones}

Les réseaux de neurones trouvent des applications dans de nombreux domaines :

\begin{itemize}
    \item \textbf{Vision par ordinateur} : Reconnaissance d'images, détection d'objets
    \item \textbf{Traitement du langage naturel} : Traduction automatique, analyse de sentiment
    \item \textbf{Recommandation} : Systèmes de recommandation personnalisés
    \item \textbf{Prédiction} : Prévisions financières, météorologiques
    \item \textbf{Classification} : Diagnostic médical, détection de fraude
    \item \textbf{Régression} : Estimation de valeurs continues
\end{itemize}

\section{Objectifs Pédagogiques et Techniques du Projet}

\subsection{Objectifs Pédagogiques}

\begin{enumerate}
    \item Comprendre le fonctionnement théorique des réseaux de neurones artificiels
    \item Visualiser l'architecture et le flux de données dans un réseau
    \item Expérimenter avec différents paramètres d'apprentissage
    \item Analyser l'évolution de l'erreur pendant l'entraînement
    \item Comprendre l'impact des fonctions d'activation sur les performances
\end{enumerate}

\subsection{Objectifs Techniques}

\begin{enumerate}
    \item Implémenter un réseau de neurones multicouche from scratch en C++
    \item Développer une interface graphique intuitive avec Qt
    \item Intégrer des algorithmes d'apprentissage (backpropagation, momentum)
    \item Fournir des outils de visualisation et d'analyse
    \item Assurer la persistance des réseaux et des résultats
\end{enumerate}

\newpage
\chapter{Analyse et Spécification du Projet}

\section{Présentation du Projet NeuroUIT}

\subsection{Vision du Projet}

NeuroUIT est un simulateur complet de réseaux de neurones artificiels développé pour répondre aux besoins pédagogiques et expérimentaux. Le projet vise à offrir une plateforme interactive permettant de créer, entraîner, visualiser et analyser des réseaux de neurones multicouches.

\subsection{Caractéristiques Principales}

Le système NeuroUIT offre les fonctionnalités suivantes :

\begin{itemize}
    \item \textbf{Création et configuration} : Interface intuitive pour définir l'architecture du réseau
    \item \textbf{Gestion des données} : Chargement de datasets au format CSV
    \item \textbf{Entraînement interactif} : Configuration des paramètres d'apprentissage avec visualisation en temps réel
    \item \textbf{Visualisation} : Représentation graphique de l'architecture et de l'évolution des performances
    \item \textbf{Analyse} : Métriques détaillées (précision, rappel, F1-score, MSE, MAE, R²)
    \item \textbf{Persistance} : Sauvegarde et chargement des réseaux entraînés
    \item \textbf{Export} : Exportation des résultats pour analyse externe
\end{itemize}

\section{Analyse des Besoins}

\subsection{Besoins Fonctionnels}

\subsubsection{Gestion du Réseau}
\begin{itemize}
    \item Créer un réseau avec une architecture personnalisable
    \item Définir le nombre de neurones par couche
    \item Choisir les fonctions d'activation pour chaque couche
    \item Visualiser l'architecture du réseau
    \item Sauvegarder et charger des réseaux
\end{itemize}

\subsubsection{Gestion des Données}
\begin{itemize}
    \item Charger des datasets au format CSV
    \item Définir le nombre d'entrées et de sorties
    \item Gérer les fichiers avec ou sans en-têtes
    \item Afficher les statistiques du dataset
    \item Calculer la matrice de corrélation
\end{itemize}

\subsubsection{Entraînement}
\begin{itemize}
    \item Configurer les paramètres d'entraînement (taux d'apprentissage, époques, batch size, momentum)
    \item Lancer l'entraînement avec possibilité d'arrêt
    \item Visualiser l'évolution de l'erreur en temps réel
    \item Afficher la progression de l'entraînement
\end{itemize}

\subsubsection{Analyse et Visualisation}
\begin{itemize}
    \item Tester le réseau sur les données de test
    \item Afficher les métriques de performance
    \item Visualiser la matrice de confusion
    \item Afficher les graphiques de prédictions vs valeurs réelles
    \item Exporter les résultats
\end{itemize}

\subsection{Besoins Non-Fonctionnels}

\begin{itemize}
    \item \textbf{Performance} : Entraînement efficace même pour des réseaux de taille moyenne
    \item \textbf{Utilisabilité} : Interface intuitive et ergonomique
    \item \textbf{Maintenabilité} : Code modulaire et bien documenté
    \item \textbf{Extensibilité} : Architecture permettant l'ajout de nouvelles fonctionnalités
    \item \textbf{Portabilité} : Compatibilité avec Windows (extensible à Linux/Mac)
\end{itemize}

\section{Cahier des Charges}

\subsection{Objectifs du Projet}

Le projet NeuroUIT doit permettre :

\begin{enumerate}
    \item La création interactive de réseaux de neurones multicouches
    \item L'entraînement de ces réseaux sur des datasets personnalisés
    \item La visualisation en temps réel de l'apprentissage
    \item L'analyse complète des performances
    \item La persistance des modèles entraînés
\end{enumerate}

\subsection{Contraintes Techniques}

\begin{itemize}
    \item \textbf{Langage} : C++17
    \item \textbf{Framework} : Qt 6.10.1 (Core, Widgets, Charts)
    \item \textbf{Plateforme} : Windows 10/11 (testé)
    \item \textbf{Compilateur} : MinGW 13.1.0 ou compatible C++17
    \item \textbf{Architecture} : Modèle MVC (Model-View-Controller)
\end{itemize}

\subsection{Contraintes Fonctionnelles}

\begin{itemize}
    \item Support des réseaux multicouches avec un nombre variable de couches
    \item Support de 4 fonctions d'activation (Sigmoid, Tanh, ReLU, Linéaire)
    \item Format de données CSV standard
    \item Format de sauvegarde propriétaire (.nui) basé sur JSON
    \item Interface graphique complète et intuitive
\end{itemize}

\section{Technologies Utilisées}

\subsection{Technologies de Développement}

\subsubsection{C++17}
Le langage C++ a été choisi pour :
\begin{itemize}
    \item Ses performances élevées, essentielles pour les calculs numériques
    \item Son contrôle fin de la mémoire
    \item Sa compatibilité avec Qt
    \item Sa capacité à gérer des structures de données complexes
\end{itemize}

\subsubsection{Qt Framework}
Qt 6.10.1 est utilisé pour :
\begin{itemize}
    \item \textbf{Qt Core} : Fonctionnalités de base (signaux/slots, gestion de fichiers)
    \item \textbf{Qt Widgets} : Interface graphique (fenêtres, boutons, layouts)
    \item \textbf{Qt Charts} : Visualisation graphique (courbes, graphiques)
\end{itemize}

\subsection{Bibliothèques et Outils}

\begin{itemize}
    \item \textbf{Standard Library C++} : Structures de données, algorithmes
    \item \textbf{Qt MOC (Meta-Object Compiler)} : Système de signaux/slots
    \item \textbf{MinGW} : Compilateur C++ pour Windows
    \item \textbf{Qt Creator} : Environnement de développement intégré
\end{itemize}

\section{Architecture Générale du Système}

\subsection{Architecture Modulaire}

Le système NeuroUIT suit une architecture modulaire organisée en plusieurs modules :

\begin{enumerate}
    \item \textbf{Module Core} : Implémentation du réseau de neurones (Neuron, Layer, Network, ActivationFunction)
    \item \textbf{Module Training} : Algorithmes d'entraînement (Trainer)
    \item \textbf{Module Dataset} : Gestion des données (DatasetManager)
    \item \textbf{Module Persistence} : Sauvegarde et chargement (Persistence)
    \item \textbf{Module UI} : Interface graphique (MainWindow, widgets de visualisation)
    \item \textbf{Module Controller} : Contrôleur MVC (Controller)
\end{enumerate}

\subsection{Pattern de Conception}

Le projet utilise le pattern \textbf{Model-View-Controller (MVC)} :

\begin{itemize}
    \item \textbf{Model} : Modules Core, Training, Dataset (logique métier)
    \item \textbf{View} : Module UI (interface utilisateur)
    \item \textbf{Controller} : Module Controller (coordination entre Model et View)
\end{itemize}

Cette architecture permet une séparation claire des responsabilités et facilite la maintenance et l'extension du code.

\newpage
\chapter{Modélisation et Conception du Système}

\section{Choix de l'Architecture Logicielle}

\subsection{Architecture en Couches}

Le système NeuroUIT suit une architecture en couches bien définie :

\begin{enumerate}
    \item \textbf{Couche Présentation} : Interface graphique (Qt Widgets)
    \item \textbf{Couche Contrôle} : Contrôleur MVC coordonnant les interactions
    \item \textbf{Couche Métier} : Logique du réseau de neurones et de l'entraînement
    \item \textbf{Couche Données} : Gestion des datasets et persistance
\end{enumerate}

\subsection{Organisation des Modules}

La structure du projet reflète l'organisation modulaire :

\begin{verbatim}
NeuroUIT/
├── include/
│   ├── core/           # Cœur du réseau neuronal
│   ├── training/       # Module d'entraînement
│   ├── dataset/        # Gestion des données
│   ├── persistence/    # Sauvegarde/chargement
│   ├── ui/             # Interface graphique
│   └── controller/     # Contrôleur MVC
├── src/                # Implémentations
├── data/               # Datasets d'exemple
└── build/              # Fichiers de compilation
\end{verbatim}

\section{Modélisation UML du Système}

\subsection{Diagramme de Classes Principal}

Le diagramme de classes présente l'architecture complète du système. Les principales classes sont organisées en modules :

\subsubsection{Module Core}

\begin{itemize}
    \item \textbf{Neuron} : Représente un neurone avec ses poids, biais et fonction d'activation
    \item \textbf{Layer} : Représente une couche de neurones
    \item \textbf{Network} : Représente le réseau complet avec ses couches
    \item \textbf{ActivationFunction} : Classe abstraite pour les fonctions d'activation
    \item Classes dérivées : \texttt{SigmoidFunction}, \texttt{TanhFunction}, \texttt{ReLUFunction}, \texttt{LinearFunction}
\end{itemize}

\subsubsection{Module Training}

\begin{itemize}
    \item \textbf{Trainer} : Gère l'entraînement du réseau avec backpropagation
    \item \textbf{TrainingParams} : Structure contenant les paramètres d'entraînement
\end{itemize}

\subsubsection{Module Dataset}

\begin{itemize}
    \item \textbf{DatasetManager} : Gère le chargement et le traitement des datasets CSV
    \item \textbf{Sample} : Structure représentant un échantillon (entrées + sorties)
    \item \textbf{Statistics} : Structure contenant les statistiques du dataset
\end{itemize}

\subsubsection{Module UI}

\begin{itemize}
    \item \textbf{MainWindow} : Fenêtre principale de l'application
    \item \textbf{NetworkConfigDialog} : Dialogue de configuration du réseau
    \item \textbf{DatasetLoadDialog} : Dialogue de chargement de dataset
    \item \textbf{TrainingParamsDialog} : Dialogue de configuration de l'entraînement
    \item \textbf{NetworkVisualizer} : Widget de visualisation du réseau
    \item \textbf{ErrorChartWidget} : Widget d'affichage de la courbe d'erreur
    \item \textbf{MetricsWidget} : Widget d'affichage des métriques
    \item \textbf{ConfusionMatrixWidget} : Widget de la matrice de confusion
    \item \textbf{ResultsDashboardWidget} : Tableau de bord des résultats
    \item \textbf{AIAssistantDialog} : Assistant IA interactif
\end{itemize}

\subsubsection{Module Controller}

\begin{itemize}
    \item \textbf{Controller} : Contrôleur principal coordonnant tous les modules
\end{itemize}

\subsection{Relations entre Classes}

Les principales relations sont :

\begin{itemize}
    \item \textbf{Composition} : Network contient des Layer, Layer contient des Neuron
    \item \textbf{Agrégation} : Controller utilise Network, Trainer, DatasetManager
    \item \textbf{Héritage} : Les fonctions d'activation héritent de ActivationFunction
    \item \textbf{Association} : MainWindow utilise Controller, les widgets utilisent Controller
\end{itemize}

\section{Description des Principaux Modules}

\subsection{Module Core (Réseau Neuronal)}

\subsubsection{Classe Neuron}

La classe \texttt{Neuron} représente l'unité fondamentale du réseau :

\begin{lstlisting}[language=C++, caption=Structure de la classe Neuron]
class Neuron {
    std::vector<double> weights_;      // Poids des connexions
    double bias_;                      // Biais
    double output_;                    // Sortie calculée
    double netInput_;                  // Somme avant activation
    double delta_;                     // Dérivée de l'erreur
    std::shared_ptr<ActivationFunction> activation_;
    
    double forward(const std::vector<double>& inputs);
    void updateWeights(const std::vector<double>& inputs, 
                       double learningRate, double momentum);
};
\end{lstlisting}

\subsubsection{Classe Layer}

La classe \texttt{Layer} représente une couche de neurones :

\begin{lstlisting}[language=C++, caption=Structure de la classe Layer]
class Layer {
    std::vector<Neuron> neurons_;
    ActivationType activationType_;
    
    std::vector<double> forward(const std::vector<double>& inputs);
    void backward(const std::vector<double>& deltas);
};
\end{lstlisting}

\subsubsection{Classe Network}

La classe \texttt{Network} représente le réseau complet :

\begin{lstlisting}[language=C++, caption=Structure de la classe Network]
class Network {
    std::vector<Layer> layers_;
    std::vector<ActivationType> activationTypes_;
    
    std::vector<double> forward(const std::vector<double>& inputs);
    double backward(const std::vector<double>& targets);
    void updateWeights(double learningRate, double momentum);
};
\end{lstlisting}

\subsection{Module Training}

\subsubsection{Classe Trainer}

La classe \texttt{Trainer} gère l'entraînement :

\begin{lstlisting}[language=C++, caption=Structure de la classe Trainer]
class Trainer {
    std::shared_ptr<Network> network_;
    
    TrainingResults train(const std::vector<Sample>& samples,
                          const TrainingParams& params,
                          ProgressCallback callback);
    double trainEpoch(const std::vector<Sample>& samples,
                      const TrainingParams& params);
};
\end{lstlisting}

\subsection{Module Dataset}

\subsubsection{Classe DatasetManager}

La classe \texttt{DatasetManager} gère les données :

\begin{lstlisting}[language=C++, caption=Structure de la classe DatasetManager]
class DatasetManager {
    std::vector<Sample> samples_;
    size_t numInputs_;
    size_t numOutputs_;
    
    bool loadFromCSV(const std::string& filename,
                     size_t numInputs, size_t numOutputs,
                     bool hasHeader);
    Statistics computeStatistics() const;
};
\end{lstlisting}

\subsection{Module UI et Visualisation}

\subsubsection{Architecture de l'Interface}

L'interface est organisée en trois panneaux principaux :

\begin{enumerate}
    \item \textbf{Panneau gauche} : Configuration (réseau, dataset, paramètres)
    \item \textbf{Panneau central} : Visualisation du réseau avec zoom
    \item \textbf{Panneau droit} : Résultats et analyses (onglets multiples)
\end{enumerate}

\subsubsection{Widgets de Visualisation}

\begin{itemize}
    \item \textbf{NetworkVisualizer} : Affiche l'architecture avec zoom interactif
    \item \textbf{ErrorChartWidget} : Courbe d'évolution de l'erreur
    \item \textbf{MetricsWidget} : Métriques de performance
    \item \textbf{ConfusionMatrixWidget} : Matrice de confusion
    \item \textbf{PredictionsChartWidget} : Graphique prédictions vs réelles
    \item \textbf{ResultsDashboardWidget} : Tableau de bord complet
\end{itemize}

\subsection{Module Controller}

\subsubsection{Classe Controller}

Le \texttt{Controller} coordonne tous les modules :

\begin{lstlisting}[language=C++, caption=Structure de la classe Controller]
class Controller : public QObject {
    std::shared_ptr<Network> network_;
    std::shared_ptr<Trainer> trainer_;
    DatasetManager datasetManager_;
    
    bool createNetwork(const std::vector<size_t>& architecture,
                       const std::vector<ActivationType>& activations);
    void startTraining(const TrainingParams& params);
    TestResults getDetailedTestResults();
};
\end{lstlisting}

\section{Diagrammes de Séquence}

\subsection{Séquence de Création d'un Réseau}

\begin{enumerate}
    \item L'utilisateur ouvre le dialogue de configuration
    \item L'utilisateur définit l'architecture
    \item MainWindow appelle Controller::createNetwork()
    \item Controller crée une instance de Network
    \item Network initialise les couches et neurones
    \item Controller émet le signal networkCreated()
    \item MainWindow met à jour l'interface
\end{enumerate}

\subsection{Séquence d'Entraînement}

\begin{enumerate}
    \item L'utilisateur configure les paramètres d'entraînement
    \item L'utilisateur lance l'entraînement
    \item Controller::startTraining() est appelé
    \item Trainer::train() est exécuté dans un thread
    \item À chaque époque, Trainer émet un callback
    \item Controller émet trainingProgress()
    \item MainWindow met à jour la courbe d'erreur
    \item À la fin, Controller émet trainingFinished()
\end{enumerate}

\newpage
\chapter{Implémentation et Realisation}

\section{Environnement de Développement}

\subsection{Configuration}

\begin{itemize}
    \item \textbf{Système d'exploitation} : Windows 10/11
    \item \textbf{IDE} : Qt Creator 6.10.1
    \item \textbf{Compilateur} : MinGW 13.1.0 (GCC)
    \item \textbf{Version C++} : C++17
    \item \textbf{Qt Version} : Qt 6.10.1
    \item \textbf{Modules Qt} : Core, Widgets, Charts
\end{itemize}

\subsection{Structure du Projet}

Le projet utilise le système de build de Qt (qmake) avec le fichier \texttt{NeuroUIT.pro} définissant :
\begin{itemize}
    \item Les headers et sources
    \item Les chemins d'inclusion
    \item Les répertoires de sortie
    \item Les configurations spécifiques à Windows
\end{itemize}

\section{Choix Technologiques}

\subsection{Justification des Choix}

\subsubsection{C++17}

Le choix de C++17 permet :
\begin{itemize}
    \item L'utilisation de \texttt{std::shared\_ptr} pour la gestion mémoire
    \item Les lambdas pour les callbacks
    \item Les structures de données modernes de la STL
    \item Les performances optimales pour les calculs numériques
\end{itemize}

\subsubsection{Qt Framework}

Qt offre :
\begin{itemize}
    \item Une interface graphique riche et moderne
    \item Le système de signaux/slots pour la communication asynchrone
    \item Les widgets de visualisation (Qt Charts)
    \item La portabilité entre plateformes
    \item Une documentation complète
\end{itemize}

\section{Implémentation du Réseau de Neurones}

\subsection{Initialisation des Poids}

Les poids sont initialisés selon une distribution normale avec moyenne 0 et écart-type 1 :

\begin{lstlisting}[language=C++, caption=Initialisation des poids]
void Neuron::initializeWeights(std::mt19937& generator, 
                                double mean, double stddev) {
    std::normal_distribution<double> dist(mean, stddev);
    for (auto& weight : weights_) {
        weight = dist(generator);
    }
    bias_ = dist(generator);
}
\end{lstlisting}

\subsection{Propagation Avant}

L'implémentation de la propagation avant :

\begin{lstlisting}[language=C++, caption=Propagation avant]
std::vector<double> Network::forward(
    const std::vector<double>& inputs) {
    std::vector<double> current = inputs;
    
    for (size_t i = 0; i < layers_.size(); ++i) {
        current = layers_[i].forward(current);
    }
    
    lastOutput_ = current;
    return current;
}
\end{lstlisting}

\subsection{Rétropropagation}

L'implémentation de la rétropropagation calcule les gradients et met à jour les poids :

\begin{lstlisting}[language=C++, caption=Rétropropagation]
double Network::backward(const std::vector<double>& targets) {
    // Calcul de l'erreur de sortie
    std::vector<double> outputErrors;
    for (size_t i = 0; i < lastOutput_.size(); ++i) {
        outputErrors.push_back(lastOutput_[i] - targets[i]);
    }
    
    // Propagation arrière
    std::vector<double> currentErrors = outputErrors;
    for (int i = layers_.size() - 1; i >= 0; --i) {
        currentErrors = layers_[i].backward(currentErrors);
    }
    
    // Calcul de l'erreur quadratique moyenne
    double mse = 0.0;
    for (double error : outputErrors) {
        mse += error * error;
    }
    return mse / outputErrors.size();
}
\end{lstlisting}

\section{Implémentation des Fonctions d'Activation}

Chaque fonction d'activation implémente l'interface \texttt{ActivationFunction} :

\begin{lstlisting}[language=C++, caption=Interface ActivationFunction]
class ActivationFunction {
public:
    virtual double activate(double x) = 0;
    virtual double derivative(double x) = 0;
};
\end{lstlisting}

\subsection{Implémentation de la Sigmoïde}

\begin{lstlisting}[language=C++, caption=Implémentation Sigmoïde]
double SigmoidFunction::activate(double x) {
    return 1.0 / (1.0 + std::exp(-x));
}

double SigmoidFunction::derivative(double x) {
    double s = activate(x);
    return s * (1.0 - s);
}
\end{lstlisting}

\section{Implémentation des Algorithmes d'Apprentissage}

\subsection{Entraînement par Époques}

L'entraînement se fait par époques, chaque époque parcourant tous les échantillons :

\begin{lstlisting}[language=C++, caption=Entraînement par époque]
double Trainer::trainEpoch(const std::vector<Sample>& samples,
                          const TrainingParams& params) {
    double totalError = 0.0;
    
    // Mélange si demandé
    auto shuffled = samples;
    if (params.shuffle) {
        std::shuffle(shuffled.begin(), shuffled.end(), generator_);
    }
    
    // Traitement par batch
    for (size_t i = 0; i < shuffled.size(); i += params.batchSize) {
        auto batch = getBatch(shuffled, i, params.batchSize);
        processBatch(batch, params);
    }
    
    return totalError / samples.size();
}
\end{lstlisting}

\subsection{Mise à Jour avec Momentum}

La mise à jour des poids utilise le momentum pour accélérer la convergence :

\begin{lstlisting}[language=C++, caption=Mise à jour avec momentum]
void Neuron::updateWeights(const std::vector<double>& inputs,
                          double learningRate, double momentum,
                          std::vector<double>& prevUpdates) {
    for (size_t i = 0; i < weights_.size(); ++i) {
        double update = -learningRate * delta_ * inputs[i];
        if (momentum > 0.0 && i < prevUpdates.size()) {
            update += momentum * prevUpdates[i];
        }
        weights_[i] += update;
        prevUpdates[i] = update;
    }
    bias_ += -learningRate * delta_;
}
\end{lstlisting}

\section{Implémentation de l'Interface Graphique}

\subsection{Architecture de MainWindow}

La fenêtre principale est organisée avec des \texttt{QSplitter} pour permettre le redimensionnement :

\begin{lstlisting}[language=C++, caption=Structure de MainWindow]
class MainWindow : public QMainWindow {
    QSplitter* mainSplitter_;      // Splitter principal horizontal
    QWidget* leftPanel_;            // Configuration
    QWidget* centerPanel_;         // Visualisation
    QWidget* rightPanel_;          // Résultats
    
    NetworkVisualizer* networkVisualizer_;
    ErrorChartWidget* errorChartWidget_;
    MetricsWidget* metricsWidget_;
    // ... autres widgets
};
\end{lstlisting}

\subsection{Widget de Visualisation du Réseau}

Le \texttt{NetworkVisualizer} dessine le réseau avec Qt Painter :

\begin{lstlisting}[language=C++, caption=Visualisation du réseau]
void NetworkVisualizer::drawNetwork(QPainter& painter) {
    // Calcul des positions des neurones
    // Dessin des connexions
    // Dessin des neurones avec couleurs par type de couche
    // Affichage des labels
}
\end{lstlisting}

Le widget supporte le zoom interactif avec des boutons +/- pour ajuster la taille de visualisation.

\subsection{Système de Signaux/Slots}

La communication entre les composants utilise le système de signaux/slots de Qt :

\begin{lstlisting}[language=C++, caption=Connexions signaux/slots]
connect(controller_.get(), SIGNAL(trainingProgress(size_t, double)),
        this, SLOT(onTrainingProgress(size_t, double)));
connect(controller_.get(), SIGNAL(trainingFinished()),
        this, SLOT(onTrainingFinished()));
\end{lstlisting}

\section{Gestion des Configurations et des Poids}

\subsection{Sauvegarde au Format .nui}

Le format .nui utilise JSON pour stocker :
\begin{itemize}
    \item L'architecture (nombre de neurones par couche)
    \item Les types de fonctions d'activation
    \item Les poids de toutes les connexions
    \item Les biais de tous les neurones
\end{itemize}

\subsection{Chargement d'un Réseau}

Le chargement restaure complètement l'état du réseau, permettant de reprendre l'entraînement ou d'utiliser un modèle pré-entraîné.

\newpage
\chapter{Simulation, Expérimentation et Résultats}

\section{Scénarios de Simulation}

\subsection{Dataset XOR}

Le problème XOR est un classique pour tester les réseaux de neurones. Il nécessite au moins une couche cachée car il n'est pas linéairement séparable.

\textbf{Configuration} :
\begin{itemize}
    \item Architecture : 2-2-1 (2 entrées, 2 neurones cachés, 1 sortie)
    \item Fonction d'activation : Sigmoïde pour les deux couches
    \item Taux d'apprentissage : 0.01
    \item Nombre d'époques : 100
    \item Batch size : 1
\end{itemize}

\textbf{Résultats attendus} : Le réseau doit apprendre la table de vérité XOR avec une erreur proche de 0.

\subsection{Dataset de Régression Linéaire}

Pour tester la capacité de régression du réseau.

\textbf{Configuration} :
\begin{itemize}
    \item Architecture : 1-5-1
    \item Fonction d'activation : ReLU pour la couche cachée, Linéaire pour la sortie
    \item Taux d'apprentissage : 0.001
    \item Nombre d'époques : 200
\end{itemize}

\section{Processus d'Entraînement et de Validation}

\subsection{Étapes d'Utilisation}

\begin{enumerate}
    \item \textbf{Création du réseau} : L'utilisateur définit l'architecture via le dialogue de configuration
    \item \textbf{Chargement du dataset} : Import d'un fichier CSV avec spécification du nombre d'entrées/sorties
    \item \textbf{Configuration de l'entraînement} : Définition des hyperparamètres
    \item \textbf{Lancement} : Démarrage de l'entraînement avec visualisation en temps réel
    \item \textbf{Analyse} : Consultation des métriques et résultats après entraînement
\end{enumerate}

\subsection{Visualisation en Temps Réel}

Pendant l'entraînement, l'interface affiche :
\begin{itemize}
    \item La courbe d'évolution de l'erreur (époques vs erreur)
    \item La progression dans la barre de statut
    \item Les logs détaillés dans l'onglet Logs
\end{itemize}

\section{Indicateurs de Performance (KPIs)}

\subsection{Métriques de Classification}

\begin{itemize}
    \item \textbf{Accuracy (Précision)} : Pourcentage de prédictions correctes
    \item \textbf{Precision} : Précision des prédictions positives
    \item \textbf{Recall (Rappel)} : Capacité à détecter les vrais positifs
    \item \textbf{F1-Score} : Moyenne harmonique de précision et rappel
\end{itemize}

\subsection{Métriques de Régression}

\begin{itemize}
    \item \textbf{MSE (Mean Squared Error)} : Erreur quadratique moyenne
    \item \textbf{MAE (Mean Absolute Error)} : Erreur absolue moyenne
    \item \textbf{R² (Coefficient de Détermination)} : Qualité de l'ajustement
\end{itemize}

\section{Analyse des Résultats Obtenus}

\subsection{Exemple : Résolution du Problème XOR}

Avec une architecture 2-2-1 et 100 époques :
\begin{itemize}
    \item L'erreur diminue progressivement de ~0.25 à ~0.01
    \item La précision atteint 100\% après convergence
    \item Le réseau apprend correctement la fonction XOR
\end{itemize}

\subsection{Impact des Hyperparamètres}

\begin{itemize}
    \item \textbf{Taux d'apprentissage trop élevé} : Oscillations, pas de convergence
    \item \textbf{Taux d'apprentissage trop faible} : Convergence très lente
    \item \textbf{Momentum} : Accélère la convergence et stabilise l'apprentissage
    \item \textbf{Batch size} : Impacte la stabilité et la vitesse d'apprentissage
\end{itemize}

\section{Discussion et Interprétation}

\subsection{Points Forts}

\begin{itemize}
    \item Interface intuitive et complète
    \item Visualisation claire de l'architecture et des performances
    \item Métriques détaillées pour l'analyse
    \item Support de différents types de problèmes (classification, régression)
    \item Assistant IA intégré pour l'aide à l'utilisation
\end{itemize}

\subsection{Limites et Améliorations Possibles}

\begin{itemize}
    \item Support limité aux réseaux feedforward (pas de réseaux récurrents)
    \item Pas d'optimiseurs avancés (Adam, RMSprop)
    \item Pas de régularisation (dropout, L1/L2)
    \item Interface limitée à Windows (portabilité à améliorer)
\end{itemize}

\section{Captures d'Écran et Descriptions}

\subsection{Capture 1 : Interface Principale}

\textbf{Description} : L'interface principale présente trois panneaux :
\begin{itemize}
    \item Panneau gauche : Configuration du réseau et du dataset
    \item Panneau central : Visualisation interactive du réseau avec zoom
    \item Panneau droit : Résultats et analyses avec onglets multiples
\end{itemize}

\textbf{Fonctionnalités visibles} :
\begin{itemize}
    \item Boutons de contrôle (Nouveau réseau, Charger dataset, Lancer)
    \item Visualisation du réseau avec couleurs par type de couche
    \item Onglets de résultats (Résultats, Erreur, Métriques, Confusion, etc.)
\end{itemize}

\subsection{Capture 2 : Configuration du Réseau}

\textbf{Description} : Le dialogue de configuration permet de :
\begin{itemize}
    \item Définir le nombre de neurones par couche
    \item Choisir la fonction d'activation pour chaque couche
    \item Ajouter/supprimer des couches
    \item Visualiser l'architecture en cours de construction
\end{itemize}

\subsection{Capture 3 : Chargement de Dataset}

\textbf{Description} : Le dialogue de chargement permet de :
\begin{itemize}
    \item Sélectionner un fichier CSV
    \item Spécifier le nombre d'entrées et de sorties
    \item Indiquer si le fichier contient des en-têtes
    \item Afficher un aperçu des données chargées
\end{itemize}

\subsection{Capture 4 : Paramètres d'Entraînement}

\textbf{Description} : Configuration des hyperparamètres :
\begin{itemize}
    \item Taux d'apprentissage (0.001 à 1.0)
    \item Nombre d'époques
    \item Batch size
    \item Momentum
    \item Option de mélange (shuffle)
\end{itemize}

\subsection{Capture 5 : Visualisation du Réseau}

\textbf{Description} : Le visualiseur affiche :
\begin{itemize}
    \item Les neurones d'entrée en bleu
    \item Les neurones cachés en vert
    \item Les neurones de sortie en rouge
    \item Les connexions entre neurones
    \item Les labels indiquant le nombre de neurones par couche
    \item Boutons de zoom (+/-) pour ajuster la taille
\end{itemize}

\subsection{Capture 6 : Courbe d'Erreur}

\textbf{Description} : Le graphique montre :
\begin{itemize}
    \item L'évolution de l'erreur en fonction des époques
    \item Une courbe décroissante indiquant l'apprentissage
    \item Mise à jour en temps réel pendant l'entraînement
\end{itemize}

\subsection{Capture 7 : Métriques de Performance}

\textbf{Description} : Affichage des métriques :
\begin{itemize}
    \item Pour classification : Accuracy, Precision, Recall, F1-Score
    \item Pour régression : MSE, MAE, R²
    \item Barres de progression visuelles
    \item Valeurs numériques précises
\end{itemize}

\subsection{Capture 8 : Matrice de Confusion}

\textbf{Description} : Visualisation de la matrice de confusion :
\begin{itemize}
    \item Tableau coloré montrant les vrais/faux positifs/négatifs
    \item Pourcentage de chaque catégorie
    \item Identification des erreurs de classification
\end{itemize}

\subsection{Capture 9 : Graphique Prédictions vs Réelles}

\textbf{Description} : Comparaison visuelle :
\begin{itemize}
    \item Points représentant les prédictions
    \item Ligne représentant les valeurs réelles
    \item Évaluation de la qualité des prédictions
\end{itemize}

\subsection{Capture 10 : Assistant IA}

\textbf{Description} : L'assistant IA interactif offre :
\begin{itemize}
    \item Catégories de questions (Introduction, Fonctionnement, Architecture, etc.)
    \item Questions cliquables
    \item Réponses détaillées et explicatives
    \item Interface intuitive pour l'apprentissage
\end{itemize}

\newpage
\chapter*{Conclusion Générale}
\addcontentsline{toc}{chapter}{Conclusion Générale}

Le projet NeuroUIT représente une réalisation complète et pédagogique dans le domaine de la simulation de réseaux de neurones artificiels. À travers ce travail, nous avons développé un outil interactif permettant de comprendre, visualiser et expérimenter avec les réseaux de neurones multicouches.

\section{Apports du Projet}

\subsection{Apports Pédagogiques}

Le projet offre une plateforme d'apprentissage interactive permettant aux étudiants de :
\begin{itemize}
    \item Visualiser concrètement le fonctionnement des réseaux de neurones
    \item Expérimenter avec différents paramètres et architectures
    \item Comprendre l'impact des hyperparamètres sur l'apprentissage
    \item Analyser les performances à travers des métriques détaillées
\end{itemize}

\subsection{Apports Techniques}

Sur le plan technique, le projet démontre :
\begin{itemize}
    \item Une implémentation from scratch d'un réseau de neurones en C++
    \item Une architecture modulaire et maintenable
    \item Une interface graphique moderne avec Qt
    \item Une intégration réussie de visualisations interactives
\end{itemize}

\section{Difficultés Rencontrées et Solutions}

\subsection{Difficultés Techniques}

\begin{itemize}
    \item \textbf{Gestion de la mémoire} : Résolue avec l'utilisation de \texttt{std::shared\_ptr}
    \item \textbf{Synchronisation threads} : Gérée avec les signaux/slots de Qt
    \item \textbf{Performance des calculs} : Optimisée avec des structures de données efficaces
    \item \textbf{Visualisation complexe} : Réalisée avec Qt Painter et gestion du zoom
\end{itemize}

\subsection{Solutions Apportées}

\begin{itemize}
    \item Architecture MVC pour la séparation des responsabilités
    \item Utilisation de callbacks pour la progression de l'entraînement
    \item Design modulaire facilitant les extensions
    \item Documentation complète du code
\end{itemize}

\section{Perspectives Futures}

\subsection{Améliorations Possibles}

\begin{itemize}
    \item Support des réseaux récurrents (RNN, LSTM)
    \item Implémentation d'optimiseurs avancés (Adam, RMSprop)
    \item Ajout de techniques de régularisation (dropout, batch normalization)
    \item Support de la parallélisation GPU
    \item Extension à d'autres plateformes (Linux, macOS)
    \item Export vers des formats standards (ONNX, TensorFlow)
\end{itemize}

\subsection{Extensions Pédagogiques}

\begin{itemize}
    \item Tutoriels interactifs intégrés
    \item Exemples de cas d'usage plus variés
    \item Comparaison automatique de différentes architectures
    \item Mode démonstration pour l'enseignement
\end{itemize}

\section{Conclusion}

Le projet NeuroUIT a atteint ses objectifs en fournissant un outil complet et pédagogique pour la simulation de réseaux de neurones artificiels. L'implémentation from scratch en C++ avec Qt démontre une compréhension approfondie des concepts fondamentaux de l'apprentissage automatique.

Ce travail contribue à la compréhension des réseaux de neurones et offre une base solide pour des développements futurs. L'architecture modulaire et le code bien structuré facilitent les extensions et les améliorations.

Nous espérons que ce projet servira de ressource pédagogique précieuse pour les étudiants et les chercheurs intéressés par l'intelligence artificielle et les réseaux de neurones.

\newpage
\begin{thebibliography}{99}

\bibitem{goodfellow2016}
Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.

\bibitem{rumelhart1986}
Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). Learning representations by back-propagating errors. \textit{Nature}, 323(6088), 533-536.

\bibitem{lecun2015}
LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning. \textit{Nature}, 521(7553), 436-444.

\bibitem{qt2024}
Qt Company. (2024). \textit{Qt Documentation}. \url{https://doc.qt.io/}

\bibitem{nielsen2015}
Nielsen, M. A. (2015). \textit{Neural Networks and Deep Learning}. Determination Press.

\bibitem{bishop2006}
Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.

\bibitem{haykin2009}
Haykin, S. (2009). \textit{Neural Networks and Learning Machines}. Pearson Education.

\bibitem{stroustrup2013}
Stroustrup, B. (2013). \textit{The C++ Programming Language}. Addison-Wesley Professional.

\end{thebibliography}

\newpage
\chapter*{Annexes}
\addcontentsline{toc}{chapter}{Annexes}

\section{Annexe A : Structure Complète du Projet}

\begin{verbatim}
NeuroUIT---Simulateur-de-R-seaux-Neuronaux/
├── include/
│   ├── core/
│   │   ├── ActivationFunction.h
│   │   ├── Layer.h
│   │   ├── Network.h
│   │   └── Neuron.h
│   ├── dataset/
│   │   └── DatasetManager.h
│   ├── training/
│   │   └── Trainer.h
│   ├── persistence/
│   │   └── Persistence.h
│   ├── ui/
│   │   ├── AIAssistantDialog.h
│   │   ├── ConfusionMatrixWidget.h
│   │   ├── CorrelationMatrixWidget.h
│   │   ├── DatasetLoadDialog.h
│   │   ├── ErrorChartWidget.h
│   │   ├── MainWindow.h
│   │   ├── MetricsWidget.h
│   │   ├── NetworkConfigDialog.h
│   │   ├── NetworkVisualizer.h
│   │   ├── PredictionsChartWidget.h
│   │   ├── ResultsDashboardWidget.h
│   │   ├── StatisticsWidget.h
│   │   └── TrainingParamsDialog.h
│   └── controller/
│       └── Controller.h
├── src/
│   ├── core/
│   ├── dataset/
│   ├── training/
│   ├── persistence/
│   ├── ui/
│   ├── controller/
│   └── main.cpp
├── data/
│   ├── XOR_dataset.csv
│   ├── AND_dataset.csv
│   ├── OR_dataset.csv
│   ├── linear_regression.csv
│   └── ...
├── build/
├── bin/
└── NeuroUIT.pro
\end{verbatim}

\section{Annexe B : Exemple de Dataset XOR}

\begin{verbatim}
x1,x2,y
0,0,0
0,1,1
1,0,1
1,1,0
\end{verbatim}

\section{Annexe C : Format de Sauvegarde .nui}

Le format .nui utilise JSON pour stocker :
\begin{itemize}
    \item L'architecture du réseau
    \item Les types de fonctions d'activation
    \item Les poids de toutes les connexions
    \item Les biais de tous les neurones
\end{itemize}

\section{Annexe D : Liste des Fonctionnalités}

\begin{itemize}
    \item Création et configuration de réseaux multicouches
    \item Support de 4 fonctions d'activation (Sigmoid, Tanh, ReLU, Linéaire)
    \item Chargement de datasets CSV
    \item Entraînement avec backpropagation et momentum
    \item Visualisation interactive du réseau
    \item Graphique d'évolution de l'erreur
    \item Métriques de performance complètes
    \item Matrice de confusion
    \item Matrice de corrélation
    \item Graphique prédictions vs réelles
    \item Sauvegarde/chargement de réseaux
    \item Export des résultats
    \item Assistant IA interactif
    \item Zoom interactif sur la visualisation
    \item Thèmes clair/sombre
\end{itemize}

\end{document}

